{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Lab\n",
    "### YOUR NAME HERE\n",
    "In this lab you will be experimenting with ANNs.  Let's start by importing a few things.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_113928\\1649511632.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate some dummy data from random samples in a 2D space from 4 clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 64\n",
    "variance = 0.01\n",
    "  \n",
    "# 4 clusters in a 2D space\n",
    "centers = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "   \n",
    "X, y = make_blobs(n_samples,\n",
    "                  centers=centers,\n",
    "                  cluster_std = np.sqrt(variance),\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use matplot lib to plot the clusters of the ``X`` values coloring the points according to their labels (``y``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=y, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "We will be classifying our data, so you should fill out the next two functions in the next cell to accurately classify the values.  You should not use any loops!  \n",
    "\n",
    "You should be able to achieve 100% accuracy with the provided blobs, so you should test your function by creating a second set of values with a higher variance so that your classifier is not able to achieve 100% accuracy.  Document this appropriately in the following cell(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier(X):\n",
    "    ''' This function takes a NumPy vector with 2 variables and returns a classification value (0-3)\n",
    "    '''\n",
    "        return -1\n",
    "\n",
    "def run_my_classifier(X, y):\n",
    "    ''' This function takes a vector of pairs of points, classifies each pair using my_classifier, and \n",
    "    then compares the predicted y value with the actual y values in the y variable.  It should return the \n",
    "    accuracy value for the two vectors.  \n",
    "    '''\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_my_classifier(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "We now define a function to create and train a Multi-layer Perceptron (MLP) classifier.   Calling this function will train the model and generate some print statements that show the confusion matrix output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp(X_train, y_train, X_test, y_test, out_class=4, hidden=16, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden, activation='relu'))\n",
    "    model.add(Dense(out_class, activation='softmax'))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,epochs=epochs,verbose=1)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_classes)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_classes)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets see how well the original (low variance) data can be clustered with the MLP.  First, we need to make sure we split the data into training and testing sets to identify overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Re-run the experiment at least 5 more times with different numbers of epochs in the next cell and plot the results to show the overall accuracy vs. the number of epochs.  Write a few sentences about what you observed about the relationship in the Reflection cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "Experiment in the next cell by trying different numbers of neurons in the hidden layer.  Identify the smallest number of hidden neurons you could use and still achieve high accuracy.  Create a table in markdown in the reflection section to show your experimental results.  Make sure your table adequately documents your experimental variables (hyperparameters, dataset) to enable reproducability.  Write a few statements in your reflection about the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "The low-variance blob data was easy to separate with simple classification rules.  Run a few experiments with the higher variance dataset you created and determine if the MLP or your deterministic solution could achieve better accuracy.  Describe your experiments in a table in the reflection section and write a few statements about your observations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pistachio Dataset\n",
    "The blob data experiments were interesting, but are not representative of a real-world problem.  Next, we will use data from an industrial pistachio classifier designed to identify different varities of pistachio nuts.  \n",
    "\n",
    "https://www.kaggle.com/datasets/muratkokludataset/pistachio-image-dataset\n",
    "\n",
    "\n",
    "Let's start by loading some data.  Because this is image data, we are going to use a generator to bring in the data.  This also allows us to add augmentation to the images to hopefully grow the robustness of our algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            validation_split=0.2,\n",
    "            rescale=1./255, # to bring the image range from 0..255 to 0..1\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0, # randomly zoom image \n",
    "            width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False) # randomly flip images\n",
    "train_it = datagen.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           batch_size=1,\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           shuffle=True,\n",
    "                                           subset='training')\n",
    "valid_it = datagen.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           shuffle=True,\n",
    "                                           batch_size=1,\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP code is expecting a NumPy array, so we need to build it.  This isn't the most elegant approach, but it gets the job done to allow our previous MLP code to work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "batch_index = 0\n",
    "\n",
    "while batch_index <= train_it.batch_index:\n",
    "    #iterate through the training data and build a single array\n",
    "    x_temp, y_temp = train_it.next()\n",
    "    X.append(np.squeeze(x_temp[0]))\n",
    "    y.append(np.squeeze(y_temp[0]))\n",
    "    batch_index = batch_index + 1\n",
    "\n",
    "X_train = np.asarray(X)\n",
    "y_train = np.asarray(y)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "batch_index = 0\n",
    "\n",
    "while batch_index <= valid_it.batch_index:\n",
    "    #iterate through the test data to build a single array\n",
    "    x_temp, y_temp = valid_it.next()\n",
    "    X.append(np.squeeze(x_temp[0]))\n",
    "    y.append(np.squeeze(y_temp[0]))\n",
    "    batch_index = batch_index + 1\n",
    "\n",
    "X_test = np.asarray(X)\n",
    "y_test = np.asarray(y)\n",
    "X_train_reshaped = X_train.reshape(1719,50176)\n",
    "X_test_reshaped = X_test.reshape(429,50176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a function that creates a DNN with a single hidden layer.  It includes the methods to train and evaluate the model as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_mlp(X_train, y_train, X_test, y_test, hidden=16, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        batch_size=1,epochs=epochs,\n",
    "                        verbose=1)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_probabilities)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_probabilities)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_probabilities))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calls the previous function and plots the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],30,10)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "In the following cell, execute at least 5 more experiments that experiment with the number of hidden neurons and number of epochs.  Create a table in the reflection and results section below that shows the configuration hyperparameters, total model parameters, test accuracy, and training accuracy.  Describe why you think the highest accuracy configuration outperformed your other experiments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Reflection\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "In the following cell, modify the model itself by adding one additional dense layer and one Dropout layer.  Re-use your hyperparameters from your highest accuracy run in the previous exercise and capture the results in the reflection below.  Answer the questions: \n",
    "\n",
    "1) Did more layers help? \n",
    "\n",
    "2) Did dropout affect overfitting?\n",
    "\n",
    "3) Did the total number of parameters correlate with changes in accuracy across your experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "In the following cell, experiment with using at least 3 different activation functions.  Keep other hyperparameters constant for these experiments and just change the activation functions.  In the reflection section below, record your experiments and the resulting accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "One of the limitations of dense networks is that they don't inherently take advange of spatially-associated information.  Using convolutional layers can enable the model to be able to learn more complicated features more efficiently since not all the layers are fully connected.  \n",
    "\n",
    "We start by creating a new generator for our CNN data to isolate it from previous experiments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_cnn = ImageDataGenerator(\n",
    "            validation_split=0.2,\n",
    "            rescale=1./255, # to bring the image range from 0..255 to 0..1\n",
    "            rotation_range=0.01,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.01, # randomly zoom image \n",
    "            width_shift_range=0.01,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.01,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False) # randomly flip images\n",
    "\n",
    "# Do not modify the generator parameters unless you are making significant model changes that necessitate it\n",
    "train_it_cnn = datagen_cnn.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           batch_size=32,\n",
    "                                           class_mode=\"binary\",\n",
    "                                           shuffle=True,\n",
    "                                           subset='training')\n",
    "valid_it_cnn = datagen.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           shuffle=True,\n",
    "                                           batch_size=1,\n",
    "                                           class_mode=\"binary\",\n",
    "                                           subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function to define and run our CNN model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_cnn(train_it, valid_it, cnn_epochs=10, deepness=1):\n",
    "    '''This function takes a training and validation generator, builds and trains a model\n",
    "    number of epochs is passed in as well as the deepness.\n",
    "    deepness changes the structure of the model by adding layers for larger numbers\n",
    "    '''\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(224,224,1)))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if(deepness > 1):\n",
    "        cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if(deepness > 2):\n",
    "        cnn_model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "        cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(Flatten())\n",
    "    if(deepness > 2):\n",
    "        cnn_model.add(Dense(512, activation='relu'))\n",
    "        cnn_model.add(Dropout(0.2))\n",
    "    if(deepness > 1):\n",
    "        cnn_model.add(Dense(256, activation='relu'))\n",
    "    cnn_model.add(Dense(128, activation='relu'))\n",
    "    cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    cnn_model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    history_cnn = cnn_model.fit(train_it_cnn,\n",
    "                  validation_data=valid_it_cnn,\n",
    "                  steps_per_epoch=train_it_cnn.samples/train_it_cnn.batch_size,\n",
    "                  validation_steps=valid_it_cnn.samples/valid_it_cnn.batch_size,\n",
    "                  epochs=cnn_epochs)\n",
    "    return history_cnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model and see how it does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_binary_cnn(train_it_cnn, valid_it, 10, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Run several experiments trying to improve the model accuracy by tuning hyperparameters changing the model structure (using the deepness parameter or tuning further if you like), and using data augmentation.  Your goal should be to beat your best MLP model by as much as possible.  You should be reading the training results to identify overfitting and tune your model and training accordingly.  Ask the instructor for help if you need guidance.  \n",
    "\n",
    "Capture at least 5 of your best experiments in the table in the Results section below.  You should capture enough information about each experiment to make it possible to re-create your results.  Write a few statements about what you learned through this exercise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Reflection\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
